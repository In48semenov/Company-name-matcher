{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6af4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c7d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3453d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bert.utils.data_constructor import CompanyDataset\n",
    "from src.bert.utils.criteriation import LabelSmoothingCrossEntropy\n",
    "from src.bert.utils.bert_clf_trainer import BertTrainClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff63d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28f9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/preprocess_train.csv'\n",
    "train_dataset = CompanyDataset(path_data, tokenizer) \n",
    "val_dataset = CompanyDataset(path_data, tokenizer, train=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223bfc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[  101,   100,   111,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  1611,   228,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,   180, 14088,  ...,     0,     0,     0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  101,  2861,  9915,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,   846,   205,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  5787,  8376,  ...,     0,     0,     0]]]),\n",
       " 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]]]),\n",
       " 'label': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataLoader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "valDataLoader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "next(iter(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc5f77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14779, 778)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataLoader), len(valDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615bb382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/bert-base-cased-conversational were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-cased-conversational and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight True\n",
      "bert.embeddings.position_embeddings.weight True\n",
      "bert.embeddings.token_type_embeddings.weight True\n",
      "bert.embeddings.LayerNorm.weight True\n",
      "bert.embeddings.LayerNorm.bias True\n",
      "bert.encoder.layer.0.attention.self.query.weight True\n",
      "bert.encoder.layer.0.attention.self.query.bias True\n",
      "bert.encoder.layer.0.attention.self.key.weight True\n",
      "bert.encoder.layer.0.attention.self.key.bias True\n",
      "bert.encoder.layer.0.attention.self.value.weight True\n",
      "bert.encoder.layer.0.attention.self.value.bias True\n",
      "bert.encoder.layer.0.attention.output.dense.weight True\n",
      "bert.encoder.layer.0.attention.output.dense.bias True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.0.intermediate.dense.weight True\n",
      "bert.encoder.layer.0.intermediate.dense.bias True\n",
      "bert.encoder.layer.0.output.dense.weight True\n",
      "bert.encoder.layer.0.output.dense.bias True\n",
      "bert.encoder.layer.0.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.attention.self.query.weight True\n",
      "bert.encoder.layer.1.attention.self.query.bias True\n",
      "bert.encoder.layer.1.attention.self.key.weight True\n",
      "bert.encoder.layer.1.attention.self.key.bias True\n",
      "bert.encoder.layer.1.attention.self.value.weight True\n",
      "bert.encoder.layer.1.attention.self.value.bias True\n",
      "bert.encoder.layer.1.attention.output.dense.weight True\n",
      "bert.encoder.layer.1.attention.output.dense.bias True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.intermediate.dense.weight True\n",
      "bert.encoder.layer.1.intermediate.dense.bias True\n",
      "bert.encoder.layer.1.output.dense.weight True\n",
      "bert.encoder.layer.1.output.dense.bias True\n",
      "bert.encoder.layer.1.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.attention.self.query.weight True\n",
      "bert.encoder.layer.2.attention.self.query.bias True\n",
      "bert.encoder.layer.2.attention.self.key.weight True\n",
      "bert.encoder.layer.2.attention.self.key.bias True\n",
      "bert.encoder.layer.2.attention.self.value.weight True\n",
      "bert.encoder.layer.2.attention.self.value.bias True\n",
      "bert.encoder.layer.2.attention.output.dense.weight True\n",
      "bert.encoder.layer.2.attention.output.dense.bias True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.intermediate.dense.weight True\n",
      "bert.encoder.layer.2.intermediate.dense.bias True\n",
      "bert.encoder.layer.2.output.dense.weight True\n",
      "bert.encoder.layer.2.output.dense.bias True\n",
      "bert.encoder.layer.2.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.attention.self.query.weight True\n",
      "bert.encoder.layer.3.attention.self.query.bias True\n",
      "bert.encoder.layer.3.attention.self.key.weight True\n",
      "bert.encoder.layer.3.attention.self.key.bias True\n",
      "bert.encoder.layer.3.attention.self.value.weight True\n",
      "bert.encoder.layer.3.attention.self.value.bias True\n",
      "bert.encoder.layer.3.attention.output.dense.weight True\n",
      "bert.encoder.layer.3.attention.output.dense.bias True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.intermediate.dense.weight True\n",
      "bert.encoder.layer.3.intermediate.dense.bias True\n",
      "bert.encoder.layer.3.output.dense.weight True\n",
      "bert.encoder.layer.3.output.dense.bias True\n",
      "bert.encoder.layer.3.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.attention.self.query.weight True\n",
      "bert.encoder.layer.4.attention.self.query.bias True\n",
      "bert.encoder.layer.4.attention.self.key.weight True\n",
      "bert.encoder.layer.4.attention.self.key.bias True\n",
      "bert.encoder.layer.4.attention.self.value.weight True\n",
      "bert.encoder.layer.4.attention.self.value.bias True\n",
      "bert.encoder.layer.4.attention.output.dense.weight True\n",
      "bert.encoder.layer.4.attention.output.dense.bias True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.intermediate.dense.weight True\n",
      "bert.encoder.layer.4.intermediate.dense.bias True\n",
      "bert.encoder.layer.4.output.dense.weight True\n",
      "bert.encoder.layer.4.output.dense.bias True\n",
      "bert.encoder.layer.4.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.attention.self.query.weight True\n",
      "bert.encoder.layer.5.attention.self.query.bias True\n",
      "bert.encoder.layer.5.attention.self.key.weight True\n",
      "bert.encoder.layer.5.attention.self.key.bias True\n",
      "bert.encoder.layer.5.attention.self.value.weight True\n",
      "bert.encoder.layer.5.attention.self.value.bias True\n",
      "bert.encoder.layer.5.attention.output.dense.weight True\n",
      "bert.encoder.layer.5.attention.output.dense.bias True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.intermediate.dense.weight True\n",
      "bert.encoder.layer.5.intermediate.dense.bias True\n",
      "bert.encoder.layer.5.output.dense.weight True\n",
      "bert.encoder.layer.5.output.dense.bias True\n",
      "bert.encoder.layer.5.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.attention.self.query.weight True\n",
      "bert.encoder.layer.6.attention.self.query.bias True\n",
      "bert.encoder.layer.6.attention.self.key.weight True\n",
      "bert.encoder.layer.6.attention.self.key.bias True\n",
      "bert.encoder.layer.6.attention.self.value.weight True\n",
      "bert.encoder.layer.6.attention.self.value.bias True\n",
      "bert.encoder.layer.6.attention.output.dense.weight True\n",
      "bert.encoder.layer.6.attention.output.dense.bias True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.intermediate.dense.weight True\n",
      "bert.encoder.layer.6.intermediate.dense.bias True\n",
      "bert.encoder.layer.6.output.dense.weight True\n",
      "bert.encoder.layer.6.output.dense.bias True\n",
      "bert.encoder.layer.6.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.attention.self.query.weight True\n",
      "bert.encoder.layer.7.attention.self.query.bias True\n",
      "bert.encoder.layer.7.attention.self.key.weight True\n",
      "bert.encoder.layer.7.attention.self.key.bias True\n",
      "bert.encoder.layer.7.attention.self.value.weight True\n",
      "bert.encoder.layer.7.attention.self.value.bias True\n",
      "bert.encoder.layer.7.attention.output.dense.weight True\n",
      "bert.encoder.layer.7.attention.output.dense.bias True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.intermediate.dense.weight True\n",
      "bert.encoder.layer.7.intermediate.dense.bias True\n",
      "bert.encoder.layer.7.output.dense.weight True\n",
      "bert.encoder.layer.7.output.dense.bias True\n",
      "bert.encoder.layer.7.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.attention.self.query.weight True\n",
      "bert.encoder.layer.8.attention.self.query.bias True\n",
      "bert.encoder.layer.8.attention.self.key.weight True\n",
      "bert.encoder.layer.8.attention.self.key.bias True\n",
      "bert.encoder.layer.8.attention.self.value.weight True\n",
      "bert.encoder.layer.8.attention.self.value.bias True\n",
      "bert.encoder.layer.8.attention.output.dense.weight True\n",
      "bert.encoder.layer.8.attention.output.dense.bias True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.intermediate.dense.weight True\n",
      "bert.encoder.layer.8.intermediate.dense.bias True\n",
      "bert.encoder.layer.8.output.dense.weight True\n",
      "bert.encoder.layer.8.output.dense.bias True\n",
      "bert.encoder.layer.8.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.attention.self.query.weight True\n",
      "bert.encoder.layer.9.attention.self.query.bias True\n",
      "bert.encoder.layer.9.attention.self.key.weight True\n",
      "bert.encoder.layer.9.attention.self.key.bias True\n",
      "bert.encoder.layer.9.attention.self.value.weight True\n",
      "bert.encoder.layer.9.attention.self.value.bias True\n",
      "bert.encoder.layer.9.attention.output.dense.weight True\n",
      "bert.encoder.layer.9.attention.output.dense.bias True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.intermediate.dense.weight True\n",
      "bert.encoder.layer.9.intermediate.dense.bias True\n",
      "bert.encoder.layer.9.output.dense.weight True\n",
      "bert.encoder.layer.9.output.dense.bias True\n",
      "bert.encoder.layer.9.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.attention.self.query.weight True\n",
      "bert.encoder.layer.10.attention.self.query.bias True\n",
      "bert.encoder.layer.10.attention.self.key.weight True\n",
      "bert.encoder.layer.10.attention.self.key.bias True\n",
      "bert.encoder.layer.10.attention.self.value.weight True\n",
      "bert.encoder.layer.10.attention.self.value.bias True\n",
      "bert.encoder.layer.10.attention.output.dense.weight True\n",
      "bert.encoder.layer.10.attention.output.dense.bias True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.intermediate.dense.weight True\n",
      "bert.encoder.layer.10.intermediate.dense.bias True\n",
      "bert.encoder.layer.10.output.dense.weight True\n",
      "bert.encoder.layer.10.output.dense.bias True\n",
      "bert.encoder.layer.10.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.attention.self.query.weight True\n",
      "bert.encoder.layer.11.attention.self.query.bias True\n",
      "bert.encoder.layer.11.attention.self.key.weight True\n",
      "bert.encoder.layer.11.attention.self.key.bias True\n",
      "bert.encoder.layer.11.attention.self.value.weight True\n",
      "bert.encoder.layer.11.attention.self.value.bias True\n",
      "bert.encoder.layer.11.attention.output.dense.weight True\n",
      "bert.encoder.layer.11.attention.output.dense.bias True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.intermediate.dense.weight True\n",
      "bert.encoder.layer.11.intermediate.dense.bias True\n",
      "bert.encoder.layer.11.output.dense.weight True\n",
      "bert.encoder.layer.11.output.dense.bias True\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_INIT)\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, params.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a056339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight False\n",
      "bert.embeddings.position_embeddings.weight False\n",
      "bert.embeddings.token_type_embeddings.weight False\n",
      "bert.embeddings.LayerNorm.weight False\n",
      "bert.embeddings.LayerNorm.bias False\n",
      "bert.encoder.layer.0.attention.self.query.weight False\n",
      "bert.encoder.layer.0.attention.self.query.bias False\n",
      "bert.encoder.layer.0.attention.self.key.weight False\n",
      "bert.encoder.layer.0.attention.self.key.bias False\n",
      "bert.encoder.layer.0.attention.self.value.weight False\n",
      "bert.encoder.layer.0.attention.self.value.bias False\n",
      "bert.encoder.layer.0.attention.output.dense.weight False\n",
      "bert.encoder.layer.0.attention.output.dense.bias False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.0.intermediate.dense.weight False\n",
      "bert.encoder.layer.0.intermediate.dense.bias False\n",
      "bert.encoder.layer.0.output.dense.weight False\n",
      "bert.encoder.layer.0.output.dense.bias False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.attention.self.query.weight False\n",
      "bert.encoder.layer.1.attention.self.query.bias False\n",
      "bert.encoder.layer.1.attention.self.key.weight False\n",
      "bert.encoder.layer.1.attention.self.key.bias False\n",
      "bert.encoder.layer.1.attention.self.value.weight False\n",
      "bert.encoder.layer.1.attention.self.value.bias False\n",
      "bert.encoder.layer.1.attention.output.dense.weight False\n",
      "bert.encoder.layer.1.attention.output.dense.bias False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.intermediate.dense.weight False\n",
      "bert.encoder.layer.1.intermediate.dense.bias False\n",
      "bert.encoder.layer.1.output.dense.weight False\n",
      "bert.encoder.layer.1.output.dense.bias False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.attention.self.query.weight False\n",
      "bert.encoder.layer.2.attention.self.query.bias False\n",
      "bert.encoder.layer.2.attention.self.key.weight False\n",
      "bert.encoder.layer.2.attention.self.key.bias False\n",
      "bert.encoder.layer.2.attention.self.value.weight False\n",
      "bert.encoder.layer.2.attention.self.value.bias False\n",
      "bert.encoder.layer.2.attention.output.dense.weight False\n",
      "bert.encoder.layer.2.attention.output.dense.bias False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.intermediate.dense.weight False\n",
      "bert.encoder.layer.2.intermediate.dense.bias False\n",
      "bert.encoder.layer.2.output.dense.weight False\n",
      "bert.encoder.layer.2.output.dense.bias False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.attention.self.query.weight False\n",
      "bert.encoder.layer.3.attention.self.query.bias False\n",
      "bert.encoder.layer.3.attention.self.key.weight False\n",
      "bert.encoder.layer.3.attention.self.key.bias False\n",
      "bert.encoder.layer.3.attention.self.value.weight False\n",
      "bert.encoder.layer.3.attention.self.value.bias False\n",
      "bert.encoder.layer.3.attention.output.dense.weight False\n",
      "bert.encoder.layer.3.attention.output.dense.bias False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.intermediate.dense.weight False\n",
      "bert.encoder.layer.3.intermediate.dense.bias False\n",
      "bert.encoder.layer.3.output.dense.weight False\n",
      "bert.encoder.layer.3.output.dense.bias False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.attention.self.query.weight False\n",
      "bert.encoder.layer.4.attention.self.query.bias False\n",
      "bert.encoder.layer.4.attention.self.key.weight False\n",
      "bert.encoder.layer.4.attention.self.key.bias False\n",
      "bert.encoder.layer.4.attention.self.value.weight False\n",
      "bert.encoder.layer.4.attention.self.value.bias False\n",
      "bert.encoder.layer.4.attention.output.dense.weight False\n",
      "bert.encoder.layer.4.attention.output.dense.bias False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.intermediate.dense.weight False\n",
      "bert.encoder.layer.4.intermediate.dense.bias False\n",
      "bert.encoder.layer.4.output.dense.weight False\n",
      "bert.encoder.layer.4.output.dense.bias False\n",
      "bert.encoder.layer.4.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.attention.self.query.weight False\n",
      "bert.encoder.layer.5.attention.self.query.bias False\n",
      "bert.encoder.layer.5.attention.self.key.weight False\n",
      "bert.encoder.layer.5.attention.self.key.bias False\n",
      "bert.encoder.layer.5.attention.self.value.weight False\n",
      "bert.encoder.layer.5.attention.self.value.bias False\n",
      "bert.encoder.layer.5.attention.output.dense.weight False\n",
      "bert.encoder.layer.5.attention.output.dense.bias False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.intermediate.dense.weight False\n",
      "bert.encoder.layer.5.intermediate.dense.bias False\n",
      "bert.encoder.layer.5.output.dense.weight False\n",
      "bert.encoder.layer.5.output.dense.bias False\n",
      "bert.encoder.layer.5.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.attention.self.query.weight False\n",
      "bert.encoder.layer.6.attention.self.query.bias False\n",
      "bert.encoder.layer.6.attention.self.key.weight False\n",
      "bert.encoder.layer.6.attention.self.key.bias False\n",
      "bert.encoder.layer.6.attention.self.value.weight False\n",
      "bert.encoder.layer.6.attention.self.value.bias False\n",
      "bert.encoder.layer.6.attention.output.dense.weight False\n",
      "bert.encoder.layer.6.attention.output.dense.bias False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.intermediate.dense.weight False\n",
      "bert.encoder.layer.6.intermediate.dense.bias False\n",
      "bert.encoder.layer.6.output.dense.weight False\n",
      "bert.encoder.layer.6.output.dense.bias False\n",
      "bert.encoder.layer.6.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.attention.self.query.weight False\n",
      "bert.encoder.layer.7.attention.self.query.bias False\n",
      "bert.encoder.layer.7.attention.self.key.weight False\n",
      "bert.encoder.layer.7.attention.self.key.bias False\n",
      "bert.encoder.layer.7.attention.self.value.weight False\n",
      "bert.encoder.layer.7.attention.self.value.bias False\n",
      "bert.encoder.layer.7.attention.output.dense.weight False\n",
      "bert.encoder.layer.7.attention.output.dense.bias False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.intermediate.dense.weight False\n",
      "bert.encoder.layer.7.intermediate.dense.bias False\n",
      "bert.encoder.layer.7.output.dense.weight False\n",
      "bert.encoder.layer.7.output.dense.bias False\n",
      "bert.encoder.layer.7.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.attention.self.query.weight False\n",
      "bert.encoder.layer.8.attention.self.query.bias False\n",
      "bert.encoder.layer.8.attention.self.key.weight False\n",
      "bert.encoder.layer.8.attention.self.key.bias False\n",
      "bert.encoder.layer.8.attention.self.value.weight False\n",
      "bert.encoder.layer.8.attention.self.value.bias False\n",
      "bert.encoder.layer.8.attention.output.dense.weight False\n",
      "bert.encoder.layer.8.attention.output.dense.bias False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.intermediate.dense.weight False\n",
      "bert.encoder.layer.8.intermediate.dense.bias False\n",
      "bert.encoder.layer.8.output.dense.weight False\n",
      "bert.encoder.layer.8.output.dense.bias False\n",
      "bert.encoder.layer.8.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.attention.self.query.weight False\n",
      "bert.encoder.layer.9.attention.self.query.bias False\n",
      "bert.encoder.layer.9.attention.self.key.weight False\n",
      "bert.encoder.layer.9.attention.self.key.bias False\n",
      "bert.encoder.layer.9.attention.self.value.weight False\n",
      "bert.encoder.layer.9.attention.self.value.bias False\n",
      "bert.encoder.layer.9.attention.output.dense.weight False\n",
      "bert.encoder.layer.9.attention.output.dense.bias False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.intermediate.dense.weight False\n",
      "bert.encoder.layer.9.intermediate.dense.bias False\n",
      "bert.encoder.layer.9.output.dense.weight False\n",
      "bert.encoder.layer.9.output.dense.bias False\n",
      "bert.encoder.layer.9.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.attention.self.query.weight False\n",
      "bert.encoder.layer.10.attention.self.query.bias False\n",
      "bert.encoder.layer.10.attention.self.key.weight False\n",
      "bert.encoder.layer.10.attention.self.key.bias False\n",
      "bert.encoder.layer.10.attention.self.value.weight False\n",
      "bert.encoder.layer.10.attention.self.value.bias False\n",
      "bert.encoder.layer.10.attention.output.dense.weight False\n",
      "bert.encoder.layer.10.attention.output.dense.bias False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.intermediate.dense.weight False\n",
      "bert.encoder.layer.10.intermediate.dense.bias False\n",
      "bert.encoder.layer.10.output.dense.weight False\n",
      "bert.encoder.layer.10.output.dense.bias False\n",
      "bert.encoder.layer.10.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.attention.self.query.weight True\n",
      "bert.encoder.layer.11.attention.self.query.bias True\n",
      "bert.encoder.layer.11.attention.self.key.weight True\n",
      "bert.encoder.layer.11.attention.self.key.bias True\n",
      "bert.encoder.layer.11.attention.self.value.weight True\n",
      "bert.encoder.layer.11.attention.self.value.bias True\n",
      "bert.encoder.layer.11.attention.output.dense.weight True\n",
      "bert.encoder.layer.11.attention.output.dense.bias True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.intermediate.dense.weight True\n",
      "bert.encoder.layer.11.intermediate.dense.bias True\n",
      "bert.encoder.layer.11.output.dense.weight True\n",
      "bert.encoder.layer.11.output.dense.bias True\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "for params in model.bert.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.bert.encoder.layer[11].parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.bert.pooler.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, params.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85cfd6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f770c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "NUM_EPOCHS = 5\n",
    "LR = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38368284",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer=optimizer, \n",
    "    max_lr=LR, \n",
    "    steps_per_epoch=len(trainDataLoader), \n",
    "    epochs=NUM_EPOCHS, \n",
    "    pct_start=0.1, \n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "criteriation = LabelSmoothingCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f116e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1986: 100%|| 14779/14779 [12:38<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/778 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1990: 100%|| 778/778 [00:35<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.770\n",
      "f1_macro_val: 0.964\n",
      "\n",
      "EPOCH 2 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/14779 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1985: 100%|| 14779/14779 [12:42<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/778 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1986: 100%|| 778/778 [00:35<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.966\n",
      "f1_macro_val: 0.974\n",
      "Save best model.\n",
      "\n",
      "EPOCH 3 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/14779 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1985: 100%|| 14779/14779 [12:21<00:00, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/778 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1985: 100%|| 778/778 [00:35<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.982\n",
      "f1_macro_val: 0.975\n",
      "Save best model.\n",
      "\n",
      "EPOCH 4 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/14779 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1985: 100%|| 14779/14779 [12:25<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/778 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1986: 100%|| 778/778 [00:35<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.989\n",
      "f1_macro_val: 0.976\n",
      "\n",
      "EPOCH 5 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/14779 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1985: 100%|| 14779/14779 [12:25<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/778 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1985: 100%|| 778/778 [00:35<00:00, 21.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.992\n",
      "f1_macro_val: 0.973\n"
     ]
    }
   ],
   "source": [
    "trainer = BertTrainClf(\n",
    "    model=model, \n",
    "    trainDataloader=trainDataLoader, \n",
    "    valDataloader=valDataLoader, \n",
    "    criteriation=criteriation,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler, \n",
    "    device=DEVICE, \n",
    "    model_name='BertNameCompany_v1'\n",
    ")\n",
    "\n",
    "results = trainer(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a4cf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss_history': [0.21360856620805965,\n",
       "  0.2005032328878061,\n",
       "  0.19963990467872694,\n",
       "  0.19922910446795278,\n",
       "  0.19904274056523874],\n",
       " 'val_loss_history': [0.20080078506224566,\n",
       "  0.2001806767058863,\n",
       "  0.20002140763639484,\n",
       "  0.2001537072896038,\n",
       "  0.20013524879220213],\n",
       " 'train_f1_history': [0.7695348772693069,\n",
       "  0.965634098781363,\n",
       "  0.9817127506209287,\n",
       "  0.9892434998683126,\n",
       "  0.992430197641661],\n",
       " 'val_f1_history': [0.9636258481378064,\n",
       "  0.974394188156579,\n",
       "  0.9745370107258291,\n",
       "  0.9761511350736807,\n",
       "  0.9734919845495044]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c01e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"log_bert_train.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadf9f7",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aceb19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, precision_recall_curve\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2935d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bert.utils.inference_bert import BertPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78283490",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)\n",
    "device='cuda:4'\n",
    "pipeline_1 = BertPipeline('../weights/BertNameCompany_v1_best.pth', tokenizer, device)\n",
    "pipeline_2 = BertPipeline('../weights/BertNameCompany_v1_last.pth', tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957af950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocess_train.csv')\n",
    "_, df_val = train_test_split(\n",
    "    df, train_size=0.95, stratify=df['is_duplicate'], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111ff82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/24891 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 24891/24891 [06:34<00:00, 63.03it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_best, pred_last = [], []\n",
    "for idx in tqdm(range(df_val.shape[0])):\n",
    "    cmp_1, cmp_2 = df_val['name_1'].iloc[idx], df_val['name_2'].iloc[idx]\n",
    "    res_1 = pipeline_1(cmp_1, cmp_2)\n",
    "    res_2 = pipeline_2(cmp_1, cmp_2)\n",
    "    pred_best.append(res_1)\n",
    "    pred_last.append(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a15a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['pred_best'] = pred_best\n",
    "df_val['pred_last'] = pred_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee1fdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9745370107258291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['is_duplicate'].tolist(), df_val['pred_best'].tolist(), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5755d11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734919845495044"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['is_duplicate'].tolist(), df_val['pred_last'].tolist(), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3f9bf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00     24708\\n           1       0.98      0.92      0.95       183\\n\\n    accuracy                           1.00     24891\\n   macro avg       0.99      0.96      0.97     24891\\nweighted avg       1.00      1.00      1.00     24891\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(df_val['is_duplicate'], df_val['pred_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f955d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00     24708\\n           1       0.96      0.93      0.95       183\\n\\n    accuracy                           1.00     24891\\n   macro avg       0.98      0.97      0.97     24891\\nweighted avg       1.00      1.00      1.00     24891\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(df_val['is_duplicate'].tolist(), df_val['pred_last'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441a953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24704,     4],\n",
       "       [   14,   169]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_val['is_duplicate'], df_val['pred_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3be4e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24701,     7],\n",
       "       [   12,   171]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_val['is_duplicate'], df_val['pred_last'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20eb3a",
   "metadata": {},
   "source": [
    "**     best**\n",
    " -        ,    \n",
    " -  presicion  label 1 = 0.98 ( 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49340397",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)\n",
    "device='cuda:4'\n",
    "\n",
    "pipeline_1 = BertPipeline('../weights/BertNameCompany_v1_best.pth', tokenizer, device, debug=True)\n",
    "\n",
    "df = pd.read_csv('../data/preprocess_train.csv')\n",
    "_, df_val = train_test_split(\n",
    "    df, train_size=0.95, stratify=df['is_duplicate'], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c8577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24891/24891 [03:12<00:00, 129.00it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for idx in tqdm(range(df_val.shape[0])):\n",
    "    cmp_1, cmp_2 = df_val['name_1'].iloc[idx], df_val['name_2'].iloc[idx]\n",
    "    res_1 = pipeline_1(cmp_1, cmp_2)\n",
    "    pred.append(res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81758d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01685393, 0.01676338, 0.01676492, ..., 1.        , 1.        ,\n",
       "        1.        ]),\n",
       " array([1.        , 0.99453552, 0.99453552, ..., 0.01092896, 0.00546448,\n",
       "        0.        ]),\n",
       " array([0.05088564, 0.05088566, 0.05088569, ..., 0.96189672, 0.96424681,\n",
       "        0.9728694 ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(df_val['is_duplicate'], pred)\n",
    "precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0258302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9962,  9963,  9964,  9965,  9966,  9967,  9968,  9969,  9970,\n",
       "         9971,  9972,  9973,  9974,  9975,  9976,  9977,  9978,  9979,\n",
       "         9980,  9981,  9982,  9983,  9984,  9985,  9986,  9987,  9988,\n",
       "         9989,  9990,  9991,  9992,  9993,  9994,  9995,  9996,  9997,\n",
       "         9998,  9999, 10000, 10001, 10002, 10003, 10004, 10005, 10006,\n",
       "        10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10015,\n",
       "        10016, 10017, 10018, 10019, 10020, 10021, 10022, 10023, 10024,\n",
       "        10025, 10026, 10027, 10028, 10029, 10101, 10102, 10103, 10104,\n",
       "        10105, 10106, 10107, 10108, 10109, 10110, 10111, 10112, 10113,\n",
       "        10114, 10115, 10116, 10117, 10118, 10119, 10120, 10121, 10122,\n",
       "        10123, 10124, 10125, 10126, 10127, 10128, 10129, 10130]),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(precision > 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73ad954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824992835521698"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[9962]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05a0e59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912568306010929"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall[9962]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "426cac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/24891 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 24891/24891 [03:12<00:00, 129.08it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for idx in tqdm(range(df_val.shape[0])):\n",
    "    cmp_1, cmp_2 = df_val['name_1'].iloc[idx], df_val['name_2'].iloc[idx]\n",
    "    res_1 = pipeline_1(cmp_1, cmp_2)\n",
    "    if res_1 > thresholds[9962]:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a40f658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741036459898735"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['is_duplicate'].tolist(), pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac7ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}