{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6af4449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append('../')\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c7d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from src.utils.data_constructor import CompanyDatasetBertClf\n",
    "from src.bert.utils.criteriation import LabelSmoothingCrossEntropy\n",
    "from src.bert.utils.bert_clf_trainer import BertTrainClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8032cd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://arts/2', creation_time=1666734835488, experiment_id='2', last_update_time=1666734835488, lifecycle_stage='active', name='company-name-matcher', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "remote_server_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(\"company-name-matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff4d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff63d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "MODEL_NAME = 'bert_name_company_v1'\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_EPOCHS = 5\n",
    "LR = 3e-5\n",
    "OPTIMIZER = 'AdamW'\n",
    "SAVE_DIR = f'../weights/{MODEL_NAME}-{datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")}'\n",
    "\n",
    "path_data = '../data/preprocess_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4c9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e33e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "    \"tokenizer\": MODEL_INIT,\n",
    "    \"batch_size\":BATCH_SIZE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"lr\": LR,\n",
    "    \"optimizer\": OPTIMIZER\n",
    "}\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28f9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)\n",
    "train_dataset = CompanyDatasetBertClf(path_data, tokenizer) \n",
    "val_dataset = CompanyDatasetBertClf(path_data, tokenizer, train=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223bfc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "valDataLoader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc5f77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataLoader), len(valDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "615bb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_INIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a056339",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for params in model.bert.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.bert.encoder.layer[11].parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.bert.pooler.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, params.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38368284",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER == \"AdamW\":\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer=optimizer, \n",
    "    max_lr=LR, \n",
    "    steps_per_epoch=len(trainDataLoader), \n",
    "    epochs=NUM_EPOCHS, \n",
    "    pct_start=0.1, \n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "criteriation = LabelSmoothingCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f116e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.2005: 100%|██████████| 60/60 [00:16<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1997: 100%|██████████| 4/4 [00:00<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "\n",
      "EPOCH 2 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1995: 100%|██████████| 60/60 [00:16<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1990: 100%|██████████| 4/4 [00:00<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n",
      "\n",
      "EPOCH 3 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1996: 100%|██████████| 60/60 [00:16<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1990: 100%|██████████| 4/4 [00:00<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n",
      "\n",
      "EPOCH 4 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1999: 100%|██████████| 60/60 [00:16<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1989: 100%|██████████| 4/4 [00:00<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n",
      "\n",
      "EPOCH 5 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1995: 100%|██████████| 60/60 [00:16<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1988: 100%|██████████| 4/4 [00:00<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n"
     ]
    }
   ],
   "source": [
    "trainer = BertTrainClf(\n",
    "    model=model, \n",
    "    trainDataloader=trainDataLoader, \n",
    "    valDataloader=valDataLoader, \n",
    "    criteriation=criteriation,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler, \n",
    "    device=DEVICE, \n",
    "    model_name=MODEL_NAME,\n",
    "    save_dir=SAVE_DIR\n",
    ")\n",
    "\n",
    "results = trainer(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2a4cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = dict()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    for keys in results:\n",
    "        log[keys] = results[keys][i]\n",
    "    mlflow.log_metrics(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c698dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"{SAVE_DIR}/log.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadf9f7",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aceb19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, precision_recall_curve\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2935d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bert.bert_inference import BertPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac77223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../weights/bert_name_company_v1-10-26-2022-05-17-54'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "DEVICE='cuda:0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78283490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)\n",
    "pipeline = BertPipeline(f'{SAVE_DIR}/best.pth', tokenizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "957af950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocess_train.csv')\n",
    "_, df_val = train_test_split(\n",
    "    df, train_size=0.95, stratify=df['is_duplicate'], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111ff82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/24891 [00:00<?, ?it/s]/home/asemenov/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████| 24891/24891 [06:34<00:00, 63.03it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_best, pred_last = [], []\n",
    "for idx in tqdm(range(df_val.shape[0])):\n",
    "    cmp_1, cmp_2 = df_val['name_1'].iloc[idx], df_val['name_2'].iloc[idx]\n",
    "    res_1 = pipeline_1(cmp_1, cmp_2)\n",
    "    res_2 = pipeline_2(cmp_1, cmp_2)\n",
    "    pred_best.append(res_1)\n",
    "    pred_last.append(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a15a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['pred_best'] = pred_best\n",
    "df_val['pred_last'] = pred_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee1fdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9745370107258291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['is_duplicate'].tolist(), df_val['pred_best'].tolist(), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5755d11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734919845495044"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['is_duplicate'].tolist(), df_val['pred_last'].tolist(), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3f9bf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00     24708\\n           1       0.98      0.92      0.95       183\\n\\n    accuracy                           1.00     24891\\n   macro avg       0.99      0.96      0.97     24891\\nweighted avg       1.00      1.00      1.00     24891\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(df_val['is_duplicate'], df_val['pred_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f955d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00     24708\\n           1       0.96      0.93      0.95       183\\n\\n    accuracy                           1.00     24891\\n   macro avg       0.98      0.97      0.97     24891\\nweighted avg       1.00      1.00      1.00     24891\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(df_val['is_duplicate'].tolist(), df_val['pred_last'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441a953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24704,     4],\n",
       "       [   14,   169]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_val['is_duplicate'], df_val['pred_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3be4e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24701,     7],\n",
       "       [   12,   171]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_val['is_duplicate'], df_val['pred_last'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20eb3a",
   "metadata": {},
   "source": [
    "**Лучше взять модель с префиксом best**\n",
    " - для данной постановки задачи лучше найти подходяющую компанию, которая точно является подходящей\n",
    " - выше presicion для label 1 = 0.98 (на 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49340397",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)\n",
    "device='cpu'\n",
    "pipeline_1 = BertPipeline('../weights/BertNameCompany_v1_best.pth', tokenizer, device, debug=True)\n",
    "\n",
    "df = pd.read_csv('../data/preprocess_train.csv')\n",
    "_, df_val = train_test_split(\n",
    "    df, train_size=0.95, stratify=df['is_duplicate'], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c8577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 24891/24891 [03:12<00:00, 129.00it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for idx in tqdm(range(df_val.shape[0])):\n",
    "    cmp_1, cmp_2 = df_val['name_1'].iloc[idx], df_val['name_2'].iloc[idx]\n",
    "    res_1 = pipeline_1(cmp_1, cmp_2)\n",
    "    pred.append(res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81758d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01685393, 0.01676338, 0.01676492, ..., 1.        , 1.        ,\n",
       "        1.        ]),\n",
       " array([1.        , 0.99453552, 0.99453552, ..., 0.01092896, 0.00546448,\n",
       "        0.        ]),\n",
       " array([0.05088564, 0.05088566, 0.05088569, ..., 0.96189672, 0.96424681,\n",
       "        0.9728694 ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(df_val['is_duplicate'], pred)\n",
    "precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0258302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9962,  9963,  9964,  9965,  9966,  9967,  9968,  9969,  9970,\n",
       "         9971,  9972,  9973,  9974,  9975,  9976,  9977,  9978,  9979,\n",
       "         9980,  9981,  9982,  9983,  9984,  9985,  9986,  9987,  9988,\n",
       "         9989,  9990,  9991,  9992,  9993,  9994,  9995,  9996,  9997,\n",
       "         9998,  9999, 10000, 10001, 10002, 10003, 10004, 10005, 10006,\n",
       "        10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10015,\n",
       "        10016, 10017, 10018, 10019, 10020, 10021, 10022, 10023, 10024,\n",
       "        10025, 10026, 10027, 10028, 10029, 10101, 10102, 10103, 10104,\n",
       "        10105, 10106, 10107, 10108, 10109, 10110, 10111, 10112, 10113,\n",
       "        10114, 10115, 10116, 10117, 10118, 10119, 10120, 10121, 10122,\n",
       "        10123, 10124, 10125, 10126, 10127, 10128, 10129, 10130]),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(precision > 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73ad954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824992835521698"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[9962]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05a0e59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912568306010929"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall[9962]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "426cac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24891 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|          | 0/24891 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for idx in tqdm(range(df_val.shape[0])):\n",
    "    cmp_1, cmp_2 = df_val['name_1'].iloc[idx], df_val['name_2'].iloc[idx]\n",
    "    res_1 = pipeline_1(cmp_1, cmp_2)\n",
    "    if res_1 > 0.912568306010929:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a40f658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741036459898735"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['is_duplicate'].tolist(), pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ac7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8235e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://arts/2', creation_time=1666734835488, experiment_id='2', last_update_time=1666734835488, lifecycle_stage='active', name='company-name-matcher', tags={}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_server_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(\"company-name-matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81d65a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"tokenizer\":'DeepPavlov/bert-base-cased-conversational',\n",
    "    \"batch_size\":32,\n",
    "    \"num_epochs\":5,\n",
    "    \"lr\":3e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c1cfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'train_loss_history': [0.21360856620805965,\n",
    "                                 0.2005032328878061,\n",
    "                                 0.19963990467872694,\n",
    "                                 0.19922910446795278,\n",
    "                                 0.19904274056523874],\n",
    "          'val_loss_history': [0.20080078506224566,\n",
    "                               0.2001806767058863,\n",
    "                               0.20002140763639484,\n",
    "                               0.2001537072896038,\n",
    "                               0.20013524879220213],\n",
    "          'train_f1_history': [0.7695348772693069,\n",
    "                               0.965634098781363,\n",
    "                               0.9817127506209287,\n",
    "                               0.9892434998683126,\n",
    "                               0.992430197641661],\n",
    "          'val_f1_history':   [0.9636258481378064,\n",
    "                               0.974394188156579,\n",
    "                               0.9745370107258291,\n",
    "                               0.9761511350736807,\n",
    "                               0.9734919845495044]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25352e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = dict()\n",
    "for i in range(5):\n",
    "    for keys in result:\n",
    "        log[keys] = result[keys][i]\n",
    "    mlflow.log_metrics(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f39e47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(pd.DataFrame(['[CLS] ' + cmp_1 + ' [SEP] ' + cmp_2 + ' [SEP]']), pd.DataFrame([res_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca35588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/26 04:20:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmptyo1ouh2/model/data, flavor: pytorch), fall back to return ['torch==1.12.1', 'cloudpickle==2.2.0']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlflow\u001b[39m.\u001b[39;49mpytorch\u001b[39m.\u001b[39;49mlog_model(pipeline_1\u001b[39m.\u001b[39;49mmodel, \u001b[39m\"\u001b[39;49m\u001b[39mbert\u001b[39;49m\u001b[39m\"\u001b[39;49m, signature\u001b[39m=\u001b[39;49msignature)\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/mlflow/pytorch/__init__.py:306\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39m    PyTorch logged models\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m pickle_module \u001b[39m=\u001b[39m pickle_module \u001b[39mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m Model\u001b[39m.\u001b[39;49mlog(\n\u001b[1;32m    307\u001b[0m     artifact_path\u001b[39m=\u001b[39;49martifact_path,\n\u001b[1;32m    308\u001b[0m     flavor\u001b[39m=\u001b[39;49mmlflow\u001b[39m.\u001b[39;49mpytorch,\n\u001b[1;32m    309\u001b[0m     pytorch_model\u001b[39m=\u001b[39;49mpytorch_model,\n\u001b[1;32m    310\u001b[0m     conda_env\u001b[39m=\u001b[39;49mconda_env,\n\u001b[1;32m    311\u001b[0m     code_paths\u001b[39m=\u001b[39;49mcode_paths,\n\u001b[1;32m    312\u001b[0m     pickle_module\u001b[39m=\u001b[39;49mpickle_module,\n\u001b[1;32m    313\u001b[0m     registered_model_name\u001b[39m=\u001b[39;49mregistered_model_name,\n\u001b[1;32m    314\u001b[0m     signature\u001b[39m=\u001b[39;49msignature,\n\u001b[1;32m    315\u001b[0m     input_example\u001b[39m=\u001b[39;49minput_example,\n\u001b[1;32m    316\u001b[0m     await_registration_for\u001b[39m=\u001b[39;49mawait_registration_for,\n\u001b[1;32m    317\u001b[0m     requirements_file\u001b[39m=\u001b[39;49mrequirements_file,\n\u001b[1;32m    318\u001b[0m     extra_files\u001b[39m=\u001b[39;49mextra_files,\n\u001b[1;32m    319\u001b[0m     pip_requirements\u001b[39m=\u001b[39;49mpip_requirements,\n\u001b[1;32m    320\u001b[0m     extra_pip_requirements\u001b[39m=\u001b[39;49mextra_pip_requirements,\n\u001b[1;32m    321\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    322\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/mlflow/models/model.py:374\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m mlflow_model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(artifact_path\u001b[39m=\u001b[39martifact_path, run_id\u001b[39m=\u001b[39mrun_id)\n\u001b[1;32m    373\u001b[0m flavor\u001b[39m.\u001b[39msave_model(path\u001b[39m=\u001b[39mlocal_path, mlflow_model\u001b[39m=\u001b[39mmlflow_model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 374\u001b[0m mlflow\u001b[39m.\u001b[39;49mtracking\u001b[39m.\u001b[39;49mfluent\u001b[39m.\u001b[39;49mlog_artifacts(local_path, mlflow_model\u001b[39m.\u001b[39;49martifact_path)\n\u001b[1;32m    375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mfluent\u001b[39m.\u001b[39m_record_logged_model(mlflow_model)\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/mlflow/tracking/fluent.py:810\u001b[0m, in \u001b[0;36mlog_artifacts\u001b[0;34m(local_dir, artifact_path)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39mLog all the contents of a local directory as artifacts of the run. If no run is active,\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[39mthis method will create a new active run.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m        mlflow.log_artifacts(\"data\", artifact_path=\"states\")\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    809\u001b[0m run_id \u001b[39m=\u001b[39m _get_or_start_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[0;32m--> 810\u001b[0m MlflowClient()\u001b[39m.\u001b[39;49mlog_artifacts(run_id, local_dir, artifact_path)\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/mlflow/tracking/client.py:1182\u001b[0m, in \u001b[0;36mMlflowClient.log_artifacts\u001b[0;34m(self, run_id, local_dir, artifact_path)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_artifacts\u001b[39m(\n\u001b[1;32m   1139\u001b[0m     \u001b[39mself\u001b[39m, run_id: \u001b[39mstr\u001b[39m, local_dir: \u001b[39mstr\u001b[39m, artifact_path: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[39m    Write a directory of files to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[39m        is_dir: True\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_artifacts(run_id, local_dir, artifact_path)\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py:469\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_artifacts\u001b[0;34m(self, run_id, local_dir, artifact_path)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_artifacts\u001b[39m(\u001b[39mself\u001b[39m, run_id, local_dir, artifact_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write a directory of files to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[1;32m    466\u001b[0m \u001b[39m    :param local_dir: Path to the directory of files to write.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m    :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_artifact_repo(run_id)\u001b[39m.\u001b[39;49mlog_artifacts(local_dir, artifact_path)\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/mlflow/store/artifact/s3_artifact_repo.py:142\u001b[0m, in \u001b[0;36mS3ArtifactRepository.log_artifacts\u001b[0;34m(self, local_dir, artifact_path)\u001b[0m\n\u001b[1;32m    140\u001b[0m     upload_path \u001b[39m=\u001b[39m posixpath\u001b[39m.\u001b[39mjoin(dest_path, rel_path)\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m filenames:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upload_file(\n\u001b[1;32m    143\u001b[0m         s3_client\u001b[39m=\u001b[39;49ms3_client,\n\u001b[1;32m    144\u001b[0m         local_file\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(root, f),\n\u001b[1;32m    145\u001b[0m         bucket\u001b[39m=\u001b[39;49mbucket,\n\u001b[1;32m    146\u001b[0m         key\u001b[39m=\u001b[39;49mposixpath\u001b[39m.\u001b[39;49mjoin(upload_path, f),\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/mlflow/store/artifact/s3_artifact_repo.py:118\u001b[0m, in \u001b[0;36mS3ArtifactRepository._upload_file\u001b[0;34m(self, s3_client, local_file, bucket, key)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m environ_extra_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     extra_args\u001b[39m.\u001b[39mupdate(environ_extra_args)\n\u001b[0;32m--> 118\u001b[0m s3_client\u001b[39m.\u001b[39;49mupload_file(Filename\u001b[39m=\u001b[39;49mlocal_file, Bucket\u001b[39m=\u001b[39;49mbucket, Key\u001b[39m=\u001b[39;49mkey, ExtraArgs\u001b[39m=\u001b[39;49mextra_args)\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/boto3/s3/inject.py:143\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    transfer.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mwith\u001b[39;00m S3Transfer(\u001b[39mself\u001b[39m, Config) \u001b[39mas\u001b[39;00m transfer:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m transfer\u001b[39m.\u001b[39;49mupload_file(\n\u001b[1;32m    144\u001b[0m         filename\u001b[39m=\u001b[39;49mFilename,\n\u001b[1;32m    145\u001b[0m         bucket\u001b[39m=\u001b[39;49mBucket,\n\u001b[1;32m    146\u001b[0m         key\u001b[39m=\u001b[39;49mKey,\n\u001b[1;32m    147\u001b[0m         extra_args\u001b[39m=\u001b[39;49mExtraArgs,\n\u001b[1;32m    148\u001b[0m         callback\u001b[39m=\u001b[39;49mCallback,\n\u001b[1;32m    149\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/boto3/s3/transfer.py:288\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    284\u001b[0m future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager\u001b[39m.\u001b[39mupload(\n\u001b[1;32m    285\u001b[0m     filename, bucket, key, extra_args, subscribers\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    289\u001b[0m \u001b[39m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m# client error.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39mexcept\u001b[39;00m ClientError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/s3transfer/futures.py:106\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel()\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[39m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[39m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coordinator\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    104\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/s3transfer/futures.py:261\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m\"\"\"Waits until TransferFuture is done and returns the result\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[39mIf the TransferFuture succeeded, it will return the result. If the\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mTransferFuture failed, it will raise the exception associated to the\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39mfailure.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# Doing a wait() with no timeout cannot be interrupted in python2 but\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# can be interrupted in python3 so we just wait with the largest\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# possible value integer value, which is on the scale of billions of\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m# years...\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_done_event\u001b[39m.\u001b[39;49mwait(MAXINT)\n\u001b[1;32m    263\u001b[0m \u001b[39m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n",
      "File \u001b[0;32m/usr/lib64/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib64/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlflow.pytorch.log_model(pipeline_1.model, \"bert\", signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9a4085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fffa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "423a779d4a8dfaa74d2e773745ab0c281fb83b66bd8f61292da672673aa4993f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
