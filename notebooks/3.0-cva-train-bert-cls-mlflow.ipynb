{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6af4449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append('../')\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380fa15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cva/Desktop/Company-name-matcher\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c7d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from src.utils.data_constructor import CompanyDatasetBertClf\n",
    "from src.bert.utils.criteriation import LabelSmoothingCrossEntropy\n",
    "from src.bert.utils.bert_clf_trainer import BertTrainClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8032cd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://arts/2', creation_time=1666734835488, experiment_id='2', last_update_time=1666734835488, lifecycle_stage='active', name='company-name-matcher', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "remote_server_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(\"company-name-matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff63d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "MODEL_NAME = 'bert'\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_EPOCHS = 5\n",
    "LR = 3e-5\n",
    "OPTIMIZER = 'AdamW'\n",
    "SAVE_DIR = f'weights/{MODEL_NAME}-{TIMESTAMP}'\n",
    "\n",
    "path_data = 'data/preprocess_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e97b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "mlflow.start_run()\n",
    "mlflow.set_tag(\"mlflow.runName\", f\"{MODEL_NAME}-{TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4c9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e33e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "    \"tokenizer\": MODEL_INIT,\n",
    "    \"batch_size\":BATCH_SIZE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"lr\": LR,\n",
    "    \"optimizer\": OPTIMIZER\n",
    "}\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28f9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)\n",
    "train_dataset = CompanyDatasetBertClf(path_data, tokenizer) \n",
    "val_dataset = CompanyDatasetBertClf(path_data, tokenizer, train=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223bfc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "valDataLoader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc5f77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataLoader), len(valDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "615bb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_INIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a056339",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for params in model.bert.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.bert.encoder.layer[11].parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.bert.pooler.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, params.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38368284",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER == \"AdamW\":\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer=optimizer, \n",
    "    max_lr=LR, \n",
    "    steps_per_epoch=len(trainDataLoader), \n",
    "    epochs=NUM_EPOCHS, \n",
    "    pct_start=0.1, \n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "criteriation = LabelSmoothingCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f116e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.2005: 100%|██████████| 60/60 [00:16<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1997: 100%|██████████| 4/4 [00:00<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "\n",
      "EPOCH 2 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1995: 100%|██████████| 60/60 [00:16<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1990: 100%|██████████| 4/4 [00:00<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n",
      "\n",
      "EPOCH 3 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1996: 100%|██████████| 60/60 [00:16<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1990: 100%|██████████| 4/4 [00:00<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n",
      "\n",
      "EPOCH 4 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1999: 100%|██████████| 60/60 [00:16<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1989: 100%|██████████| 4/4 [00:00<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n",
      "\n",
      "EPOCH 5 of 5\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1995: 100%|██████████| 60/60 [00:16<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Loss: 0.1988: 100%|██████████| 4/4 [00:00<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro_train: 0.499\n",
      "f1_macro_val: 1.000\n",
      "Save best model.\n"
     ]
    }
   ],
   "source": [
    "trainer = BertTrainClf(\n",
    "    model=model, \n",
    "    trainDataloader=trainDataLoader, \n",
    "    valDataloader=valDataLoader, \n",
    "    criteriation=criteriation,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler, \n",
    "    device=DEVICE, \n",
    "    model_name=MODEL_NAME,\n",
    "    save_dir=SAVE_DIR\n",
    ")\n",
    "\n",
    "results = trainer(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2a4cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = dict()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    for keys in results:\n",
    "        log[keys] = results[keys][i]\n",
    "    mlflow.log_metrics(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c698dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"{SAVE_DIR}/log.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_artifact(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadf9f7",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aceb19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, precision_recall_curve\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2935d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bert.bert_inference import BertPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac77223",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT = 'DeepPavlov/bert-base-cased-conversational'\n",
    "DEVICE='cuda:0'\n",
    "# SAVE_DIR = f'weights/bert-10-19-22-00-00-00'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78283490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_INIT)\n",
    "pipeline = BertPipeline(tokenizer, f'{SAVE_DIR}/best.pth', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "957af950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/preprocess_train.csv')\n",
    "_, df_val = train_test_split(\n",
    "    df, train_size=0.95, stratify=df['is_duplicate'], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "111ff82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24891 [00:00<?, ?it/s]/home/cva/Desktop/Company-name-matcher/.venv/lib64/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 24891/24891 [04:26<00:00, 93.46it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_best, pred_last = [], []\n",
    "for idx in tqdm(range(df_val.shape[0])):\n",
    "    cmp_1, cmp_2 = df_val['name_1'].iloc[idx], df_val['name_2'].iloc[idx]\n",
    "    res = pipeline(cmp_1, cmp_2)\n",
    "    pred_best.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a15a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['pred_best'] = pred_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cee1fdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9756115189143046"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['is_duplicate'].tolist(), df_val['pred_best'].tolist(), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3f9bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     24708\n",
      "           1       0.99      0.91      0.95       183\n",
      "\n",
      "    accuracy                           1.00     24891\n",
      "   macro avg       1.00      0.96      0.98     24891\n",
      "weighted avg       1.00      1.00      1.00     24891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_val['is_duplicate'], df_val['pred_best']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "441a953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24707,     1],\n",
       "       [   16,   167]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_val['is_duplicate'], df_val['pred_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81758d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00735205, 0.99404762, 1.        ]),\n",
       " array([1.        , 0.91256831, 0.        ]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(df_val['is_duplicate'], df_val['pred_best'])\n",
    "precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d389edd",
   "metadata": {},
   "source": [
    "## Save Model in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cfe2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "class FastTextWrapper(mlflow.pyfunc.PythonModel):\n",
    "    from src.bert.bert_inference import BertPipeline\n",
    "    def load_context(self, context):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/bert-base-cased-conversational\")\n",
    "        self.model = BertPipeline(tokenizer, context.artifacts[\"model_path\"], DEVICE)\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        company_1, company_2, top_n = model_input\n",
    "        res = self.model(company_1, company_2, top_n)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12e0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = {\"model_path\": f\"{SAVE_DIR}/best.pth\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53bbed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pyfunc.save_model(\n",
    "    path=f\"{SAVE_DIR}/model\",\n",
    "    python_model=FastTextWrapper(),\n",
    "    artifacts=artifacts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a0e9d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7f5ca88d79d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.pyfunc.log_model(\n",
    "    artifact_path=f\"{SAVE_DIR}/model\",\n",
    "    python_model=FastTextWrapper(),\n",
    "    artifacts=artifacts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a4085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9549bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# logged_model = 'runs:/d7c9cc785a8e4c578990893d66685d04/weights/bert-10-19-22-00-00-00/model'\n",
    "\n",
    "# loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# loaded_model.predict([\"abba\", \"abba\", 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "423a779d4a8dfaa74d2e773745ab0c281fb83b66bd8f61292da672673aa4993f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
